{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTNSw+CRmRXdEAmx0zTn3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hr7657316/CODE-A-HAUNT-hackathon-/blob/main/speech_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS_p6V3zHFny",
        "outputId": "844a0c1d-72d6-46d1-a7b1-2b89a28e0389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-7jjn0yev\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-7jjn0yev\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.6.0)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvX0gkyWH8nx",
        "outputId": "bb8ba1a7-c0e9-4d78-a556-68fc2decefa0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper  \"sample1.mp3\"  --model medium.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhB6YMsqIJHW",
        "outputId": "fe083ac2-b095-49b2-c779-79bea8e1b57f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:04.140]  Have you ever been so in love with someone else,\n",
            "[00:04.140 --> 00:05.400]  and it looks so obvious?\n",
            "[00:05.400 --> 00:07.980]  No Brother\n",
            "[00:07.980 --> 00:14.760]  4 chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper -h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ArnPjXvIJD7",
        "outputId": "c9a81be6-a208-4ffd-ecf1-7241bacb9094"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: whisper [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n",
            "               [--output_dir OUTPUT_DIR] [--output_format {txt,vtt,srt,tsv,json,all}]\n",
            "               [--verbose VERBOSE] [--task {transcribe,translate}]\n",
            "               [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
            "               [--temperature TEMPERATURE] [--best_of BEST_OF] [--beam_size BEAM_SIZE]\n",
            "               [--patience PATIENCE] [--length_penalty LENGTH_PENALTY]\n",
            "               [--suppress_tokens SUPPRESS_TOKENS] [--initial_prompt INITIAL_PROMPT]\n",
            "               [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT] [--fp16 FP16]\n",
            "               [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
            "               [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
            "               [--logprob_threshold LOGPROB_THRESHOLD] [--no_speech_threshold NO_SPEECH_THRESHOLD]\n",
            "               [--word_timestamps WORD_TIMESTAMPS] [--prepend_punctuations PREPEND_PUNCTUATIONS]\n",
            "               [--append_punctuations APPEND_PUNCTUATIONS] [--highlight_words HIGHLIGHT_WORDS]\n",
            "               [--max_line_width MAX_LINE_WIDTH] [--max_line_count MAX_LINE_COUNT]\n",
            "               [--max_words_per_line MAX_WORDS_PER_LINE] [--threads THREADS]\n",
            "               [--clip_timestamps CLIP_TIMESTAMPS]\n",
            "               [--hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD]\n",
            "               audio [audio ...]\n",
            "\n",
            "positional arguments:\n",
            "  audio                 audio file(s) to transcribe\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL         name of the Whisper model to use (default: small)\n",
            "  --model_dir MODEL_DIR\n",
            "                        the path to save model files; uses ~/.cache/whisper by default (default:\n",
            "                        None)\n",
            "  --device DEVICE       device to use for PyTorch inference (default: cuda)\n",
            "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
            "                        directory to save the outputs (default: .)\n",
            "  --output_format {txt,vtt,srt,tsv,json,all}, -f {txt,vtt,srt,tsv,json,all}\n",
            "                        format of the output file; if not specified, all available formats will be\n",
            "                        produced (default: all)\n",
            "  --verbose VERBOSE     whether to print out the progress and debug messages (default: True)\n",
            "  --task {transcribe,translate}\n",
            "                        whether to perform X->X speech recognition ('transcribe') or X->English\n",
            "                        translation ('translate') (default: transcribe)\n",
            "  --language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}\n",
            "                        language spoken in the audio, specify None to perform language detection\n",
            "                        (default: None)\n",
            "  --temperature TEMPERATURE\n",
            "                        temperature to use for sampling (default: 0)\n",
            "  --best_of BEST_OF     number of candidates when sampling with non-zero temperature (default: 5)\n",
            "  --beam_size BEAM_SIZE\n",
            "                        number of beams in beam search, only applicable when temperature is zero\n",
            "                        (default: 5)\n",
            "  --patience PATIENCE   optional patience value to use in beam decoding, as in\n",
            "                        https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to\n",
            "                        conventional beam search (default: None)\n",
            "  --length_penalty LENGTH_PENALTY\n",
            "                        optional token length penalty coefficient (alpha) as in\n",
            "                        https://arxiv.org/abs/1609.08144, uses simple length normalization by\n",
            "                        default (default: None)\n",
            "  --suppress_tokens SUPPRESS_TOKENS\n",
            "                        comma-separated list of token ids to suppress during sampling; '-1' will\n",
            "                        suppress most special characters except common punctuations (default: -1)\n",
            "  --initial_prompt INITIAL_PROMPT\n",
            "                        optional text to provide as a prompt for the first window. (default: None)\n",
            "  --condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT\n",
            "                        if True, provide the previous output of the model as a prompt for the next\n",
            "                        window; disabling may make the text inconsistent across windows, but the\n",
            "                        model becomes less prone to getting stuck in a failure loop (default:\n",
            "                        True)\n",
            "  --fp16 FP16           whether to perform inference in fp16; True by default (default: True)\n",
            "  --temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK\n",
            "                        temperature to increase when falling back when the decoding fails to meet\n",
            "                        either of the thresholds below (default: 0.2)\n",
            "  --compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD\n",
            "                        if the gzip compression ratio is higher than this value, treat the\n",
            "                        decoding as failed (default: 2.4)\n",
            "  --logprob_threshold LOGPROB_THRESHOLD\n",
            "                        if the average log probability is lower than this value, treat the\n",
            "                        decoding as failed (default: -1.0)\n",
            "  --no_speech_threshold NO_SPEECH_THRESHOLD\n",
            "                        if the probability of the <|nospeech|> token is higher than this value AND\n",
            "                        the decoding has failed due to `logprob_threshold`, consider the segment\n",
            "                        as silence (default: 0.6)\n",
            "  --word_timestamps WORD_TIMESTAMPS\n",
            "                        (experimental) extract word-level timestamps and refine the results based\n",
            "                        on them (default: False)\n",
            "  --prepend_punctuations PREPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation symbols with the next\n",
            "                        word (default: \"'“¿([{-)\n",
            "  --append_punctuations APPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation symbols with the\n",
            "                        previous word (default: \"'.。,，!！?？:：”)]}、)\n",
            "  --highlight_words HIGHLIGHT_WORDS\n",
            "                        (requires --word_timestamps True) underline each word as it is spoken in\n",
            "                        srt and vtt (default: False)\n",
            "  --max_line_width MAX_LINE_WIDTH\n",
            "                        (requires --word_timestamps True) the maximum number of characters in a\n",
            "                        line before breaking the line (default: None)\n",
            "  --max_line_count MAX_LINE_COUNT\n",
            "                        (requires --word_timestamps True) the maximum number of lines in a segment\n",
            "                        (default: None)\n",
            "  --max_words_per_line MAX_WORDS_PER_LINE\n",
            "                        (requires --word_timestamps True, no effect with --max_line_width) the\n",
            "                        maximum number of words in a segment (default: None)\n",
            "  --threads THREADS     number of threads used by torch for CPU inference; supercedes\n",
            "                        MKL_NUM_THREADS/OMP_NUM_THREADS (default: 0)\n",
            "  --clip_timestamps CLIP_TIMESTAMPS\n",
            "                        comma-separated list start,end,start,end,... timestamps (in seconds) of\n",
            "                        clips to process, where the last end timestamp defaults to the end of the\n",
            "                        file (default: 0)\n",
            "  --hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD\n",
            "                        (requires --word_timestamps True) skip silent periods longer than this\n",
            "                        threshold (in seconds) when a possible hallucination is detected (default:\n",
            "                        None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper  \"english.mp3\"  --model medium.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvUm2ig8O6SJ",
        "outputId": "bdd97db9-8ec5-4758-f71b-739794694e8d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:08.960]  Twas the night before Christmas, when all through the house, not a creature was stirring,\n",
            "[00:08.960 --> 00:10.720]  not even a mouse.\n",
            "[00:10.720 --> 00:17.360]  The stockings were hung by the chimney with care, in hopes that St. Nicholas, that's me,\n",
            "[00:17.360 --> 00:19.560]  soon would be there.\n",
            "[00:19.560 --> 00:26.040]  The children were nestled all snug in their beds, while visions of sugarplums danced in\n",
            "[00:26.040 --> 00:34.680]  their heads, and Mama in her kerchief, I in my cap, had just settled down for a long winter's\n",
            "[00:34.680 --> 00:36.560]  nap.\n",
            "[00:36.560 --> 00:43.520]  When out on the lawn there arose such a clatter, I sprang from my bed to see what was the matter.\n",
            "[00:43.520 --> 00:50.720]  Away to the window I flew like a flash, tore open the shutters, threw up the sash.\n",
            "[00:50.720 --> 00:58.660]  The moon on the breast of the new-fallen snow gave a luster of midday to objects below.\n",
            "[00:58.660 --> 01:08.840]  When what to my wandering eyes should appear, but a miniature sleigh, and eight tiny reindeer,\n",
            "[01:08.840 --> 01:18.520]  with a little old driver so lively and quick, I knew in a moment it must be St. Nick.\n",
            "[01:18.520 --> 01:23.880]  For rapid than eagles his corsairs they came, and he whistled and he shouted and he called\n",
            "[01:23.880 --> 01:25.660]  them by name.\n",
            "[01:25.660 --> 01:33.560]  Now Dasher, now Dancer, now Prancer and Vixen, on Comet, on Cupid, on Donner and Blitzen,\n",
            "[01:33.560 --> 01:36.820]  to the top of the porch, to the top of the wall.\n",
            "[01:36.820 --> 01:41.580]  Now dash away, dash away, dash away all.\n",
            "[01:41.580 --> 01:47.640]  As dry leaves before the wild hurricane fly, when they meet with an obstacle, mount to\n",
            "[01:47.640 --> 01:50.640]  the sky.\n",
            "[01:50.640 --> 01:57.880]  So up to the roof-top the corsairs they flew, with sleigh full of toys, well, and St. Nicholas\n",
            "[01:57.880 --> 01:59.960]  too.\n",
            "[01:59.960 --> 02:08.360]  And then in a twinkling I heard on the roof the prancing and pawing of each little hoof.\n",
            "[02:08.360 --> 02:14.600]  As I drew in my head and was turning around, down the chimney St. Nicholas came with the\n",
            "[02:14.600 --> 02:18.320]  bound.\n",
            "[02:18.320 --> 02:24.040]  He was dressed all in fur from his head to his foot, and his clothes were all tarnished\n",
            "[02:24.040 --> 02:26.400]  with ashes and soot.\n",
            "[02:26.400 --> 02:33.080]  A bundle of toys he had flung on his back, and he looked like a peddler just opening\n",
            "[02:33.080 --> 02:35.040]  his pack.\n",
            "[02:35.040 --> 02:36.920]  His eyes how they twinkled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper english.mp3 --language Japanese"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtKHqwuxSPYA",
        "outputId": "76092b04-00d1-4259-bfe7-bcb37afe3789"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:07<00:00, 61.2MiB/s]\n",
            "[00:00.000 --> 00:05.380] ''Twas the night before Christmas When all through the house\n",
            "[00:05.380 --> 00:10.080]  Not a creature was stirring Oh, not even a mouse\n",
            "[00:10.080 --> 00:13.740]  The stockings were hung by the chimney with care\n",
            "[00:13.740 --> 00:19.160]  In hopes that St. Nicholas That's me, soon would be there\n",
            "[00:19.160 --> 00:22.940] The children were nestled all snug in their beds\n",
            "[00:22.940 --> 00:27.080]  While visions of sugar-plums danced in their heads\n",
            "[00:27.080 --> 00:35.240] そして母だと私、私たちのまま言って、長いお昇になった website 幫助響を再運転しよう\n",
            "[00:35.300 --> 00:37.560]   foってから分担ね Tru 明日のトレ 大会が登場し\n",
            "[00:37.640 --> 00:41.000] ワクサクチケマッタラあおい半年漫才が消える\n",
            "[00:41.060 --> 00:44.140] 素晴らしいフバイ データからな\n",
            "[00:44.220 --> 00:46.360] そっかに時間を見てやろう\n",
            "[00:46.440 --> 00:48.200] さらに籠が曝ぶ 同行完成\n",
            "[00:48.280 --> 00:49.440] 空を表現するましょう\n",
            "[00:49.520 --> 00:51.400]  説明をして\n",
            "[00:51.480 --> 00:51.960] 最初は initiative 疲れたのに\n",
            "[00:52.040 --> 00:53.400] イーサル挑叉\n",
            "[00:53.460 --> 00:55.980] 次は一日時間が終了\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\", line 687, in _main_loop\n",
            "    logits = self.inference.logits(tokens, audio_features)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\", line 163, in logits\n",
            "    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/model.py\", line 211, in forward\n",
            "    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/model.py\", line 138, in forward\n",
            "    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/model.py\", line 91, in forward\n",
            "    return self.out(wv), qk\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/model.py\", line 40, in forward\n",
            "    None if self.bias is None else self.bias.to(x.dtype),\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/whisper\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 597, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 279, in transcribe\n",
            "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 195, in decode_with_fallback\n",
            "    decode_result = model.decode(segment, options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\", line 824, in decode\n",
            "    result = DecodingTask(model, options).run(mel)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\", line 737, in run\n",
            "    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\", line 708, in _main_loop\n",
            "    self.inference.cleanup_caching()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\", line 167, in cleanup_caching\n",
            "    hook.remove()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/hooks.py\", line 35, in remove\n",
            "    hooks_dict = self.hooks_dict_ref()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper english.mp3 --language hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgIBWBaZ6Jb4",
        "outputId": "01043db0-3717-415c-c515-9e33d8ea7f28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:27.260]  वीरे बाठ़ा ओर में से खेर्यता scary\n",
            "[00:27.260 --> 00:32.260]  । । । । ।\n",
            "[00:57.260 --> 01:01.260]   uninteresting eyes should appear  zoo호   Thankfully, coul l see me dreaming of those eyes\n",
            "[01:01.260 --> 01:05.260]  but a miniature slay and 8 tiny reindeer hohoho houoho cloves\n",
            "[01:05.260 --> 01:08.260]  with little old drivers so lively and quick i knew in a moment it must\n",
            "[01:08.260 --> 01:11.180]  be saint nick houoho houoho\n",
            "[01:11.180 --> 01:14.620]  more rapid than eagles his corsairs they came  completely\n",
            "[01:14.620 --> 01:18.260]  and he whistled and he shouted andy called them by name now dasher now dancer now\n",
            "[01:18.260 --> 01:21.540]  volunteer man for the kids dance he said\n",
            "[01:21.540 --> 01:24.260]  he gives presents to his requestant\n",
            "[01:24.260 --> 01:29.080]  మర్ర్వ్ర్�ోటీ్ట్క్త్త్న్టడిల్కేనిల్దేప్ల్యంచిత్రందుగ్దకోరోహీప్osse.\n",
            "[01:29.180 --> 01:31.800]  టమముడిడసి⋌లాయో approached Uncle Chelsea Gare intoler eas famous\n",
            "[01:32.280 --> 01:37.080]  కవన్త్ఖడత్న్ర్్ప్టాద్ద attachments Biology, waving\n",
            "[01:37.080 --> 02:03.040]  जशक\n",
            "[02:07.080 --> 02:26.460]  ॐू ವNEY S5ಟವ್ಯ ಬ ಂಡಿಟಣ ಬ ಸಿ ರಿಿಫತ ಥತ್ವಪ ಶ� blink ರಂಡಮferenceಡಲ್ಗೆನ್ನ ಮಂವ್�타zićಪ್ಿಶಿ್하는 ಯತ್ಸುioxidನ, ಬ Judahಮ différentಮauraisಾರೂಹಿಸudibleತಿಉಣಾಯ Cenaಗಾನ್ಪ ಮತ್ಣ ಜಾಖತ�ಾಡಿ ಱಂಂಜಿರದಿಸೂದಿವಾಖವ ಸಂದ needed..​\n",
            "[02:37.080 --> 02:37.290]  अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने अपने �\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7QnDN21P2h1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uzQuPB8sG5CV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}